{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Join the kaggle contest - https://www.kaggle.com/competitions/planttraits2024\n",
    "2. Install kaggle cli - https://github.com/Kaggle/kaggle-api/blob/main/docs/README.md\n",
    "3. Download the data - `kaggle competitions download -c planttraits2024`\n",
    "4. Unzip the data\n",
    "5. Install FGVC repo - `pip install -e .` and `pip install -r requirement.txt` in the desired env\n",
    "5. Setup data from `data_setup.ipynb`\n",
    "5. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import omegaconf\n",
    "import hydra\n",
    "from lightning import Callback, LightningDataModule, LightningModule, Trainer\n",
    "from omegaconf import DictConfig\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create this dataframe from data_setup.ipynb\n",
    "df_all = pd.read_csv('/home/ubuntu/FGVC11/data/PlantTrait/df_processed.csv')\n",
    "cfg = omegaconf.OmegaConf.load(\"/home/ubuntu/FGVC11/configs/data/plant_traits_data.yaml\")\n",
    "datamodule: LightningDataModule = hydra.utils.instantiate(cfg)\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all columns must be identical to be consider the same species\n",
    "label_col = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "aux_col = list(\n",
    "            map(lambda x: x.replace(\"mean\", \"sd\"), label_col)\n",
    "        )\n",
    "metadata_col = list(set(df_all.columns) - set(label_col) - set(aux_col) - set([\"id\", \"path\", \"split\", \"species\"]))\n",
    "len(label_col), len(aux_col), len(metadata_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[label_col][df_all.split != \"test\"].hist(bins=50, figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding CLIP feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import clip\n",
    "# from PIL import Image\n",
    "# from tqdm import tqdm\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# # model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "# model = torch.hub.load(\n",
    "#             \"facebookresearch/dinov2\", \"dinov2_vits14_reg\", pretrained=True\n",
    "#         )\n",
    "# model.to(device);\n",
    "\n",
    "# # Define the transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((490, 490)),  # Resize the image to a fixed size\n",
    "#     transforms.ToTensor(),  # Convert the image to a tensor\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "# ])\n",
    "\n",
    "# feats = []\n",
    "# for _, row in tqdm(df_train.iterrows(), total=len(df_train)):\n",
    "#     image = transform(Image.open(row[\"path\"])).unsqueeze(0).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         image_features = model(image.to(device))\n",
    "#         feats.append(image_features.view(-1).cpu().numpy())\n",
    "#     # break\n",
    "\n",
    "# feats = np.array(feats)\n",
    "# # np.save(\"feats.npy\", feats)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"feats_processed.npy\", feats)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.load(\"feats_processed.npy\")\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = pd.DataFrame(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training data\n",
    "X = df_all[metadata_col][df_all.split != \"test\"].reset_index(drop=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clip feaures and save the final test data\n",
    "x_final = df_all[metadata_col][df_all.split == \"test\"].reset_index(drop=True).values\n",
    "x_final = np.concatenate([x_final, feat_df[df_all.split == \"test\"].values], axis=1)\n",
    "\n",
    "dx_final = xgb.DMatrix(x_final, enable_categorical=False)\n",
    "# Create a DataFrame with the corresponding IDs\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': df_all[\"id\"][df_all.split == \"test\"].values,\n",
    "})\n",
    "x_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LabelEncoder for trasnforming label to log + standardization \n",
    "from fgvc.models.plant_traits_model import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasnform labels using label encoder\n",
    "Y = le.transform(torch.tensor(df_all[label_col][df_all.split != \"test\"].values)).numpy()\n",
    "Y = pd.DataFrame(Y, columns=label_col)\n",
    "Y.hist(bins=50, figsize=(10, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model for all the traits simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train  = X[df_all.split == \"train\"].values\n",
    "X_val = X[df_all.split == \"val\"].values\n",
    "y_train = Y[df_all.split == \"train\"].values\n",
    "y_val = Y[df_all.split == \"val\"].values\n",
    "\n",
    "# concatenate clip features\n",
    "X_train = np.concatenate([X_train, feat_df[df_all.split == \"train\"].values], axis=1)\n",
    "X_val = np.concatenate([X_val, feat_df[df_all.split == \"val\"].values], axis=1)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "# Create regression matrices\n",
    "dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical=False)\n",
    "dval_reg = xgb.DMatrix(X_val, y_val, enable_categorical=False)\n",
    "\n",
    "# set hyperparams\n",
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\", \"max_depth\":2}\n",
    "n = 1000\n",
    "\n",
    "evals = [(dval_reg, \"validation\")]\n",
    "\n",
    "model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain_reg,\n",
    "    num_boost_round=n,\n",
    "    evals=evals,\n",
    "    verbose_eval=20,\n",
    "    # Activate early stopping\n",
    "    early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "print(\"*\"*100)\n",
    "# get the encoded prediction\n",
    "y_train_pred = model.predict(dtrain_reg)\n",
    "# decode raw predicted traits\n",
    "y_train_pred = le.inverse_transform(torch.Tensor(y_train_pred)).numpy()\n",
    "# get the raw labels\n",
    "y_train = le.inverse_transform(torch.Tensor(y_train)).numpy()\n",
    "# evaluate\n",
    "print(f\"R2 train: {r2_score(y_train, y_train_pred)}\")\n",
    "\n",
    "# get the encoded prediction\n",
    "y_val_pred = model.predict(dval_reg)\n",
    "# decode raw predicted traits\n",
    "y_train_pred = le.inverse_transform(torch.Tensor(y_train_pred)).numpy()\n",
    "# get the raw labels\n",
    "y_train = le.inverse_transform(torch.Tensor(y_train)).numpy()\n",
    "# evaluate\n",
    "print(f\"R2 val : {r2_score(y_val, y_val_pred)}\")\n",
    "\n",
    "sub_cols = [i.replace(\"_mean\", \"\") for i in label_col]\n",
    "# get the encoded prediction\n",
    "y_final = model.predict(dx_final)\n",
    "# decode raw predicted traits\n",
    "y_final = le.inverse_transform(torch.Tensor(y_final)).numpy()\n",
    "submission_df[sub_cols] = y_final\n",
    "    \n",
    "# compare with a good submission from kaggle to sanity check\n",
    "sub = pd.read_csv('submission_good.csv')\n",
    "print(\"R2 with good\", r2_score(sub[sub.columns[1:]], submission_df[sub.columns[1:]]))\n",
    "print(\"*\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map to the best trait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('submission_good.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_traits = df_all[label_col + ['species']][df_all.split != 'test'].groupby('species').mean()\n",
    "species_traits.reset_index(inplace=True)\n",
    "species_traits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_traits = []\n",
    "for _, row in tqdm(submission_df.iterrows(), total=len(submission_df)):\n",
    "    obj_traits = []\n",
    "    for col in label_col:\n",
    "        col_sub = col.replace(\"_mean\", \"\")\n",
    "        pred_trt = row[col_sub]\n",
    "        tgt_trts = species_traits[col]\n",
    "        mapped_trait = tgt_trts[np.argmin(abs(pred_trt - tgt_trts))]\n",
    "        obj_traits.append(mapped_trait)\n",
    "        # break\n",
    "    obj_traits = np.array(obj_traits)\n",
    "    mapped_traits.append(obj_traits)\n",
    "\n",
    "mapped_traits = np.array(mapped_traits)\n",
    "\n",
    "# Create a DataFrame with the predictions and corresponding IDs\n",
    "mapped_sub = pd.DataFrame({\n",
    "    'id': submission_df['id'].values,\n",
    "    'X4': mapped_traits[:, 0],\n",
    "    'X11': mapped_traits[:, 1],\n",
    "    'X18': mapped_traits[:, 2],\n",
    "    'X50': mapped_traits[:, 3],\n",
    "    'X26': mapped_traits[:, 4],\n",
    "    'X3112': mapped_traits[:, 5],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check before mapping\n",
    "for col in label_col:\n",
    "    col = col.replace(\"_mean\", \"\")\n",
    "    print(f\"r2 {col}: {r2_score(sub[col], submission_df[col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check after mapping\n",
    "for col in label_col:\n",
    "    col = col.replace(\"_mean\", \"\")\n",
    "    print(f\"r2 {col}: {r2_score(sub[col], mapped_sub[col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[sub.columns].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c planttraits2024 -f submission_good.csv -m \"good\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each trait separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the data\n",
    "# X_train  = X[df_all.split == \"train\"].values\n",
    "# X_val = X[df_all.split == \"val\"].values\n",
    "# y_train = Y[df_all.split == \"train\"].values\n",
    "# y_val = Y[df_all.split == \"val\"].values\n",
    "\n",
    "# # concatenate clip features\n",
    "# X_train = np.concatenate([X_train, feat_df[df_all.split == \"train\"].values], axis=1)\n",
    "# X_val = np.concatenate([X_val, feat_df[df_all.split == \"val\"].values], axis=1)\n",
    "# print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "# for idx, col in enumerate(label_col):\n",
    "#     # Create regression matrices\n",
    "#     dtrain_reg = xgb.DMatrix(X_train, y_train[:, idx], enable_categorical=False)\n",
    "#     dval_reg = xgb.DMatrix(X_val, y_val[:, idx], enable_categorical=False)\n",
    "    \n",
    "#     # set hyperparams\n",
    "#     params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\", \"max_depth\":2}\n",
    "#     n = 1000\n",
    "\n",
    "#     evals = [(dval_reg, \"validation\")]\n",
    "\n",
    "#     model = xgb.train(\n",
    "#         params=params,\n",
    "#         dtrain=dtrain_reg,\n",
    "#         num_boost_round=n,\n",
    "#         evals=evals,\n",
    "#         verbose_eval=10,\n",
    "#         # Activate early stopping\n",
    "#         early_stopping_rounds=10\n",
    "#         )\n",
    "    \n",
    "#     print(\"*\"*100)\n",
    "#     y_train_pred = model.predict(dtrain_reg)\n",
    "#     y_train_pred = np.exp(SCALER.inverse_transform(y_train_pred.reshape(-1, 1)))\n",
    "#     y_train = np.exp(SCALER.inverse_transform(y_train.reshape(-1, 1)))\n",
    "#     print(f\"R2 train {col}: {r2_score(y_train, y_train_pred)}\")\n",
    "    \n",
    "#     y_val_pred = model.predict(dval_reg)\n",
    "#     y_val_pred = np.exp(SCALER.inverse_transform(y_val_pred.reshape(-1, 1)))\n",
    "#     y_val = np.exp(SCALER.inverse_transform(y_val.reshape(-1, 1)))\n",
    "#     print(f\"R2 val {col} : {r2_score(y_val, y_val_pred)}\")\n",
    "\n",
    "\n",
    "#     y_final = model.predict(dx_final)\n",
    "#     y_final = np.exp(SCALER.inverse_transform(y_final.reshape(-1, 1)))\n",
    "#     submission_df[col.replace(\"_mean\", \"\")] = y_final\n",
    "#     print(\"*\"*100)    \n",
    "#     # break\n",
    "# # compare with a good submission from kaggle to sanity check\n",
    "# sub = pd.read_csv('submission_good.csv')\n",
    "# r2_score(sub[sub.columns[1:]], submission_df[sub.columns[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure columns are in right order, otherwise you'll get absurd values\n",
    "# submission_df[sub.columns].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
