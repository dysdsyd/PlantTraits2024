{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import timm\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "import hydra\n",
    "import omegaconf\n",
    "import pyrootutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from fgvc.data.plant_clef_data import PlantCLEFDataset, PlantSPECIESDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = \"/home/ubuntu/FGVC11/data/PlantClef/pretrained_models/class_mapping.txt\"\n",
    "species_mapping = \"/home/ubuntu/FGVC11/data/PlantClef/pretrained_models/species_id_to_name.txt\"\n",
    "pretrained_path = \"/home/ubuntu/FGVC11/data/PlantClef/pretrained_models/vit_base_patch14_reg4_dinov2_lvd142m_pc24_onlyclassifier_then_all/model_best.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_class_mapping(class_list_file):\n",
    "    with open(class_list_file) as f:\n",
    "        class_index_to_class_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "    return class_index_to_class_name\n",
    "\n",
    "\n",
    "def load_species_mapping(species_map_file):\n",
    "    df = pd.read_csv(species_map_file, sep=';', quoting=1, dtype={'species_id': str})\n",
    "    df = df.set_index('species_id')\n",
    "    return  df['species'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_to_spid = load_class_mapping(class_mapping)\n",
    "spid_to_sp = load_species_mapping(species_mapping)\n",
    "    \n",
    "device = \"cuda\"\n",
    "\n",
    "model = timm.create_model('vit_base_patch14_reg4_dinov2.lvd142m', pretrained=False, \n",
    "                          num_classes=len(cid_to_spid), checkpoint_path=pretrained_path)\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feat Extraction on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/ubuntu/FGVC11/data/PlantClef/PlantCLEFTrainLQ.csv\", delimiter=\";\", escapechar=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class PlantClefDataset(Dataset):\n",
    "    def __init__(self, df, transforms):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"path\"]).convert(\"RGB\")\n",
    "        img = self.transforms(img)\n",
    "        name = row[\"image_name\"].replace(\".jpg\", \"\")\n",
    "        return img, name\n",
    "\n",
    "# Create the dataset\n",
    "dataset = PlantClefDataset(df, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# for i, (crops, coords, plot_id) in tqdm(enumerate(dataset[10:]), total=len(dataset[10:])):\n",
    "for i in range(10):\n",
    "    crops, plot_id = dataset[i+10000]\n",
    "    crops = crops.unsqueeze(0)\n",
    "    attn_maps = []\n",
    "    with torch.no_grad():\n",
    "        out, attn = model.forward_features(crops.to(device), return_attn=True)\n",
    "        class_attention = attn[:, :, 0, model.num_prefix_tokens:]\n",
    "        class_attention = class_attention.mean(1)\n",
    "        attention_map = class_attention.reshape((-1, 37, 37))\n",
    "        attention_upsampled = torch.nn.functional.interpolate(\n",
    "            attention_map.unsqueeze(1), \n",
    "            size=(518, 518), \n",
    "            mode='bilinear', \n",
    "            align_corners=False\n",
    "        ).squeeze(1).cpu()\n",
    "\n",
    "    plt.imshow(crops[0].permute(1, 2, 0).cpu().numpy(), alpha=0.9)\n",
    "    plt.imshow(attention_upsampled.cpu().numpy()[0], cmap='hot', alpha=0.6)  # overlay attention\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.global_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dirpath = \"/home/ubuntu/FGVC11/data/PlantClef/lq_feats\"\n",
    "os.makedirs(dirpath, exist_ok=True)\n",
    "for i, (data, name) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    save_path = f\"{dirpath}/{name}.pt\"\n",
    "    if os.path.exists(save_path):\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        feats = model.forward_features(data.unsqueeze(0).to(device))[:, 0].detach().cpu()\n",
    "    torch.save(feats.squeeze(0), save_path)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feat Extraction on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(columns=[\"path\", \"plot_id\", \"species_ids\"])\n",
    "submission_df[\"path\"] = glob(\"/home/ubuntu/FGVC11/data/PlantClef/images/*.jpg\")\n",
    "submission_df[\"plot_id\"] = submission_df[\"path\"].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.open(submission_df[\"path\"][0]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_crops(crops, coords, crop_size, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Visualize image crops at their respective coordinates.\n",
    "\n",
    "    Args:\n",
    "    - crops (list of PIL Images or Tensors): Image crops.\n",
    "    - coords (list of tuples): Coordinates (x, y) for each crop.\n",
    "    - figsize (tuple): Figure size for the plot.\n",
    "    \"\"\"\n",
    "    # Create a large canvas\n",
    "    max_x = max(x for x, y in coords) + crop_size\n",
    "    max_y = max(y for x, y in coords) + crop_size\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot each crop in its correct location\n",
    "    for crop, (x, y) in zip(crops, coords):\n",
    "        if isinstance(crop, torch.Tensor):\n",
    "            crop = crop.permute(1, 2, 0).numpy()  # Convert CHW tensor to HWC for visualization\n",
    "        ax.imshow(crop, extent=(x, x + crop_size, y + crop_size, y), origin='upper')\n",
    "        # Create a red rectangle around the crop\n",
    "        rect = patches.Rectangle((x, y), crop_size, crop_size, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlim(0, max_x)\n",
    "    ax.set_ylim(max_y, 0)\n",
    "    ax.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "def visualize_attn(crops, attn_maps, coords, crop_size, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Visualize image crops at their respective coordinates.\n",
    "\n",
    "    Args:\n",
    "    - crops (list of PIL Images or Tensors): Image crops.\n",
    "    - coords (list of tuples): Coordinates (x, y) for each crop.\n",
    "    - figsize (tuple): Figure size for the plot.\n",
    "    \"\"\"\n",
    "    # Create a large canvas\n",
    "    max_x = max(x for x, y in coords) + crop_size\n",
    "    max_y = max(y for x, y in coords) + crop_size\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot each crop in its correct location\n",
    "    for crop, attn, (x, y) in zip(crops, attn_maps, coords):\n",
    "        if isinstance(crop, torch.Tensor):\n",
    "            crop = crop.permute(1, 2, 0).numpy()  # Convert CHW tensor to HWC for visualization\n",
    "        ax.imshow(crop, extent=(x, x + crop_size, y + crop_size, y), origin='upper',  alpha=1)\n",
    "        ax.imshow(attn, extent=(x, x + crop_size, y + crop_size, y), cmap='hot', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlim(0, max_x)\n",
    "    ax.set_ylim(max_y, 0)\n",
    "    ax.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "def visualize_global_attn(crops, attn_maps, coords, crop_size, figsize=(15, 15)):\n",
    "    \"\"\"\n",
    "    Visualize a global attention map created by merging individual crop attention maps\n",
    "    onto a composite image reconstructed from all crops.\n",
    "\n",
    "    Args:\n",
    "    - crops (list of PIL Images or Tensors): Image crops.\n",
    "    - attn_maps (list of Tensors): Attention maps for each crop, matching the crop size.\n",
    "    - coords (list of tuples): Coordinates (x, y) for each crop.\n",
    "    - figsize (tuple): Figure size for the plot.\n",
    "    \"\"\"\n",
    "    # Determine the dimensions of the full image\n",
    "    max_x = max(x + crop_size for x, y in coords)\n",
    "    max_y = max(y + crop_size for x, y in coords)\n",
    "\n",
    "    # Create empty canvas for the full image and attention\n",
    "    full_image = np.zeros((max_y, max_x, 3), dtype=np.float32)\n",
    "    full_attention = np.zeros((max_y, max_x), dtype=np.float32)\n",
    "\n",
    "    # Assemble the full image and the corresponding attention map\n",
    "    for crop, attn, (x, y) in zip(crops, attn_maps, coords):\n",
    "        if isinstance(crop, torch.Tensor):\n",
    "            crop = crop.permute(1, 2, 0).numpy()  # Convert CHW tensor to HWC for visualization\n",
    "\n",
    "        # Place crop in the full image\n",
    "        full_image[y:y+crop_size, x:x+crop_size, :] = crop\n",
    "        \n",
    "        # Place attention map in the full attention map\n",
    "        full_attention[y:y+crop_size, x:x+crop_size] = attn.numpy()\n",
    "\n",
    "    # Visualize the results\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(full_image, extent=(0, max_x, max_y, 0))\n",
    "    attention_image = ax.imshow(full_attention, extent=(0, max_x, max_y, 0), cmap='hot', alpha=0.7)\n",
    "    \n",
    "    # Adding a color bar\n",
    "    cbar = plt.colorbar(attention_image, ax=ax, orientation='vertical')\n",
    "    cbar.set_label('Attention Intensity')\n",
    "    \n",
    "    ax.set_xlim(0, max_x)\n",
    "    ax.set_ylim(max_y, 0)\n",
    "    ax.axis('off')  # Hide axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# import torchvision.transforms as trfs\n",
    "\n",
    "# class PlantClefDataset(Dataset):\n",
    "#     def __init__(self, df, transforms, crop_size=800):\n",
    "#         self.df = df\n",
    "#         self.transforms = transforms\n",
    "#         self.crop_size = crop_size\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.df.iloc[idx]\n",
    "#         img = Image.open(row[\"path\"]).convert(\"RGB\")\n",
    "\n",
    "#         # Initialize list to hold the crops and their coordinates\n",
    "#         crops = []\n",
    "#         coords = []\n",
    "\n",
    "#         # Determine the starting point for the last crop in each dimension\n",
    "#         last_crop_start_x = max(0, img.width - self.crop_size)\n",
    "#         last_crop_start_y = max(0, img.height - self.crop_size)\n",
    "\n",
    "#         # Number of crops horizontally and vertically\n",
    "#         num_crops_x = (img.width + self.crop_size - 1) // self.crop_size\n",
    "#         num_crops_y = (img.height + self.crop_size - 1) // self.crop_size\n",
    "\n",
    "#         for i in range(num_crops_y):\n",
    "#             for j in range(num_crops_x):\n",
    "#                 # Calculate the starting points for the crops\n",
    "#                 start_x = j * self.crop_size if j != num_crops_x - 1 else last_crop_start_x\n",
    "#                 start_y = i * self.crop_size if i != num_crops_y - 1 else last_crop_start_y\n",
    "\n",
    "#                 # Crop the image\n",
    "#                 crop = img.crop((start_x, start_y, start_x + self.crop_size, start_y + self.crop_size))\n",
    "#                 crop = self.transforms(crop)\n",
    "#                 crops.append(crop)\n",
    "#                 coords.append((start_x, start_y))\n",
    "\n",
    "#         # Stack all crops into a tensor\n",
    "#         crops_tensor = torch.stack(crops)\n",
    "#         return crops_tensor, coords, row[\"plot_id\"]\n",
    "\n",
    "# # Example usage\n",
    "# # Assuming 'submission_df' and 'transforms' are defined elsewhere in your code\n",
    "# dataset = PlantClefDataset(submission_df, transforms)\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as trfs\n",
    "\n",
    "class PlantClefDataset(Dataset):\n",
    "    def __init__(self, df, transforms, min_crop_size=500, max_crop_size=600):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        self.min_crop_size = min_crop_size\n",
    "        self.max_crop_size = max_crop_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"path\"]).convert(\"RGB\")\n",
    "        \n",
    "        # Determine the best crop size within the range that covers the entire image\n",
    "        best_crop_size = self.find_best_crop_size(img.width, img.height)\n",
    "\n",
    "        # Initialize list to hold the crops and their coordinates\n",
    "        crops = []\n",
    "        coords = []\n",
    "\n",
    "        # Number of crops horizontally and vertically\n",
    "        num_crops_x = img.width // best_crop_size\n",
    "        num_crops_y = img.height // best_crop_size\n",
    "\n",
    "        for i in range(num_crops_y):\n",
    "            for j in range(num_crops_x):\n",
    "                start_x = j * best_crop_size\n",
    "                start_y = i * best_crop_size\n",
    "                crop = img.crop((start_x, start_y, start_x + best_crop_size, start_y + best_crop_size))\n",
    "                crop = self.transforms(crop)\n",
    "                crops.append(crop)\n",
    "                coords.append((start_x, start_y))\n",
    "\n",
    "        # Stack all crops into a tensor\n",
    "        crops_tensor = torch.stack(crops)\n",
    "        return crops_tensor, coords, row[\"plot_id\"], best_crop_size\n",
    "\n",
    "    def find_best_crop_size(self, width, height):\n",
    "        # Evaluate which crop size in the range has the minimum leftover area\n",
    "        best_crop_size = self.min_crop_size\n",
    "        min_leftover = width % self.min_crop_size + height % self.min_crop_size\n",
    "\n",
    "        for crop_size in range(self.min_crop_size + 1, self.max_crop_size + 1):\n",
    "            leftover = width % crop_size + height % crop_size\n",
    "            if leftover < min_leftover:\n",
    "                min_leftover = leftover\n",
    "                best_crop_size = crop_size\n",
    "\n",
    "        return best_crop_size\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'submission_df' and 'transforms' are defined elsewhere in your code\n",
    "dataset = PlantClefDataset(submission_df, transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "crops, coords, plot_id, crop_size = dataset[110]\n",
    "visualize_crops(crops, coords, 518)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# for i, (crops, coords, plot_id) in tqdm(enumerate(dataset[10:]), total=len(dataset[10:])):\n",
    "crops, coords, plot_id, crop_size = dataset[100]\n",
    "attn_maps = []\n",
    "with torch.no_grad():\n",
    "    out, attn = model.forward_features(crops.to(device), return_attn=True)\n",
    "    class_attention = attn[:, :, 0, model.num_prefix_tokens:]\n",
    "    class_attention = class_attention.mean(1)\n",
    "    attention_map = class_attention.reshape((-1, 37, 37))\n",
    "    attention_upsampled = torch.nn.functional.interpolate(\n",
    "        attention_map.unsqueeze(1), \n",
    "        size=(518, 518), \n",
    "        mode='nearest-exact', \n",
    "        # align_corners=False\n",
    "    ).squeeze(1).cpu()\n",
    "\n",
    "visualize_global_attn(crops, attention_upsampled, coords, 518)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_labels = []\n",
    "dirpath = \"/home/ubuntu/FGVC11/data/PlantClef/lq_feats\"\n",
    "os.makedirs(dirpath, exist_ok=True)\n",
    "for i, (crops, coords, name, cs) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    # save_path = f\"{dirpath}/{name}.pt\"\n",
    "    # if os.path.exists(save_path):\n",
    "    #     continue\n",
    "    with torch.no_grad():\n",
    "        out = model.forward(crops.to(device))\n",
    "        max_prob = torch.argmax(out, dim=1).cpu().numpy()\n",
    "        unique_idx = np.unique(max_prob)\n",
    "        test_labels.append(str([int(cid_to_spid[j]) for j in unique_idx]))\n",
    "        # feats = model.forward_features(data.unsqueeze(0).to(device))[:, 0].detach().cpu()\n",
    "    # torch.save(feats.squeeze(0), save_path)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"species_ids\"] = test_labels\n",
    "submission_df[[\"plot_id\", \"species_ids\"]].to_csv(\"my_run_4.csv\", sep=';', index=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_idx = np.unique(max_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_PLACEHOLDER\n",
    "from llava.conversation import conv_templates\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import re\n",
    "from llava.utils import disable_torch_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
