# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: plant_clef_data
  - override /model: plant_clef_model
  - override /callbacks: default
  - override /trainer: gpu
  - override /logger: neptune.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# tags: ["mnist", "simple_dense_net"]
description: "test_edgenext_small"

seed: 12345

trainer:
  min_epochs: 10
  max_epochs: 60
  gradient_clip_val: 0.5
  devices: [0,1,2,3]
  check_val_every_n_epoch: 1
  precision: 16-mixed
  strategy: ddp_find_unused_parameters_true



model:
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: True
    lr: 0.00011
    weight_decay: 0.01

  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: min
    factor: 0.5
    patience: 4

data:
  batch_size: 420
  num_workers: 8



logger:
  neptune:
    project: terraclear/plant-clef

callbacks:
  model_checkpoint:
    dirpath: ${paths.output_dir}/checkpoints
    filename: "epoch_{epoch:03d}"
    monitor: "val/loss"
    mode: "min"
    save_last: True
    auto_insert_metric_name: False

  early_stopping:
    monitor: "val/loss"
    patience: 5
    mode: "min"
