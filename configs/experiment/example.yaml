# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: plant_traits_data
  - override /model: plant_traits_model
  - override /callbacks: default
  - override /trainer: gpu
  - override /logger: neptune.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# tags: ["mnist", "simple_dense_net"]
description: "test dinov2"

seed: 12345

trainer:
  min_epochs: 10
  max_epochs: 60
  gradient_clip_val: 0.5
  devices: [0,1,2,3]
  check_val_every_n_epoch: 1
  precision: 16-mixed
  strategy: ddp_find_unused_parameters_true



model:
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: True
    lr: 0.00011
    weight_decay: 0.01

  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: min
    factor: 0.5
    patience: 4

data:
  batch_size: 64
  num_workers: 8



logger:
  neptune:
    project: dysdsyd/plant-traits
